{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from IPython import embed\n",
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "simply remove non alphabetical characters and lowercase everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'c', 'd']"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = [['a', 'b'],['c','d']]\n",
    "[ c for item in t for c in item]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataReader(object):\n",
    "    def __init__(self, fn:str, mode:str='word'):\n",
    "        self.mode = mode\n",
    "        self.path = Path(fn)\n",
    "        self.data = self.read_data(self.path)\n",
    "\n",
    "    def read_data(self, path:Path):\n",
    "        data = []\n",
    "        with open(path, mode='r') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            data.append(self.tokenizer(line, self.mode))\n",
    "        if self.mode == 'char':\n",
    "            return [ c for row in data for c in row ]\n",
    "        else:\n",
    "            return data\n",
    "    \n",
    "    @staticmethod\n",
    "    def tokenizer(sentence:str, mode:str='word'):\n",
    "        \n",
    "        if mode == 'word':\n",
    "            l = re.sub('[^A-Za-z]+',' ', sentence.strip().lower()).split()\n",
    "        if mode == 'char':\n",
    "            l = list(re.sub('[^A-Za-z]+',' ', sentence.strip().lower()))\n",
    "        return(l)\n",
    "    \n",
    "    def __getitem__(self, index:int):\n",
    "        return(self.data[index])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return(len(self.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = DataReader('data/35.txt', mode='char')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['p', 'r', 'o', 'j', 'e', 'c', 't', ' ', 'g', 'u', 't', 'e', 'n', 'b', 'e', 'r', 'g', ' ', 's', ' ']\n"
     ]
    }
   ],
   "source": [
    "print(d[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['t', 'h', 'i', 's', ' ', 'i', 's', ' ', 't', 'r', 'u', 'e', ' ', 'a', 't', ' ', 'l', 'a', 'e', 's', 't', ' ']\n"
     ]
    }
   ],
   "source": [
    "print(d.tokenizer('this is true!\\n 34 at laest!', mode='char'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['t',\n",
       " 'h',\n",
       " 'i',\n",
       " 's',\n",
       " ' ',\n",
       " 'i',\n",
       " 's',\n",
       " ' ',\n",
       " 't',\n",
       " 'r',\n",
       " 'u',\n",
       " 'e',\n",
       " ' ',\n",
       " 'a',\n",
       " 't',\n",
       " ' ',\n",
       " 'l',\n",
       " 'a',\n",
       " 'e',\n",
       " 's',\n",
       " 't']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['project', 'gutenberg', 's', 'the', 'time', 'machine', 'by', 'h', 'g', 'herbert', 'george', 'wells'] 3157\n"
     ]
    }
   ],
   "source": [
    "print(d[0], len(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for line in d:\n",
    "    [ tokens.append(element) for element in line]\n",
    "\n",
    "cnt = Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
